import pandas as pd
import numpy as np
import warnings
import joblib
import json
from pathlib import Path
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import cross_val_score, KFold
from sklearn.metrics import mean_squared_error
import xgboost as xgb
import lightgbm as lgb
from sklearn.ensemble import RandomForestRegressor
from utils import load_zero_mask, save_metadata

warnings.filterwarnings('ignore')


class SoilNutrientPredictor:
    """
    Multi-output regression model for soil nutrient prediction
    Handles geospatial features, zero-mask constraints, and RMSE optimization
    """
    
    NUTRIENTS = ['Al', 'B', 'Ca', 'Cu', 'Fe', 'K', 'Mg', 'Mn', 'N', 'Na', 'P', 'S', 'Zn']
    
    def __init__(self, model_type='xgboost', cv_folds=5, max_depth=8, 
                 learning_rate=0.1, n_estimators=200, random_state=42):
        """
        Initialize the predictor
        
        Args:
            model_type: 'xgboost', 'lightgbm', or 'random_forest'
            cv_folds: Number of cross-validation folds
            max_depth: Maximum tree depth
            learning_rate: Learning rate for boosting
            n_estimators: Number of estimators
            random_state: Random seed for reproducibility
        """
        self.model_type = model_type
        self.cv_folds = cv_folds
        self.random_state = random_state
        self.params = {
            'max_depth': max_depth,
            'learning_rate': learning_rate,
            'n_estimators': n_estimators,
            'random_state': random_state,
            'n_jobs': -1
        }
        self.models = {}
        self.scalers = {}
        self.feature_names = None
        self.zero_mask_dict = None
        self.training_results = {}
        
    def load_data(self, train_df, test_df, mask_df=None):
        """
        Load and prepare datasets
        
        Args:
            train_df: Training dataframe
            test_df: Test dataframe
            mask_df: TargetPred_To_Keep dataframe (optional)
        
        Returns:
            self
        """
        self.train = train_df.copy()
        self.test = test_df.copy()
        
        # Load zero-mask if provided
        if mask_df is not None:
            self.zero_mask_dict = {}
            for _, row in mask_df.iterrows():
                sample_id = row['ID']
                for i, nutrient in enumerate(self.NUTRIENTS):
                    if i + 1 < len(row):
                        if row.iloc[i + 1] == 0:
                            self.zero_mask_dict[(sample_id, nutrient)] = True
        
        return self
    
    def _engineer_features(self, df, is_train=True):
        """
        Create geospatial and derived features
        
        Args:
            df: Input dataframe
            is_train: Whether this is training data
        
        Returns:
            DataFrame with engineered features
        """
        df = df.copy()
        
        # Geospatial features
        df['lat_rad'] = np.radians(df['Latitude'])
        df['lon_rad'] = np.radians(df['Longitude'])
        df['lat_lon'] = df['Latitude'] * df['Longitude']
        df['lat_sq'] = df['Latitude'] ** 2
        df['lon_sq'] = df['Longitude'] ** 2
        
        # Depth features (parse from depth_interval column)
        if 'depth_interval' in df.columns:
            # Parse "0-20" or "20-50" format
            depth_parts = df['depth_interval'].str.split('-', expand=True)
            df['depth_upper'] = depth_parts[0].astype(float)
            df['depth_lower'] = depth_parts[1].astype(float)
            df['depth_mid'] = (df['depth_upper'] + df['depth_lower']) / 2
        elif 'horizon_upper' in df.columns and 'horizon_lower' in df.columns:
            df['depth_mid'] = (df['horizon_upper'] + df['horizon_lower']) / 2
        else:
            df['depth_mid'] = 10  # Default
        
        df['depth_log'] = np.log1p(df['depth_mid'])
        df['depth_range'] = df.get('depth_lower', df['depth_mid']) - df.get('depth_upper', 0)
        
        # Coordinate binning (regional patterns)
        df['lat_bin'] = pd.cut(df['Latitude'], bins=10, labels=False)
        df['lon_bin'] = pd.cut(df['Longitude'], bins=10, labels=False)
        
        # Interaction features
        df['lat_depth'] = df['Latitude'] * df['depth_log']
        df['lon_depth'] = df['Longitude'] * df['depth_log']
        
        # Store feature names (exclude ID and targets for training)
        if is_train:
            exclude_cols = ['ID', 'Latitude', 'Longitude', 'start_date', 'end_date',
                           'horizon_lower', 'horizon_upper', 'depth_interval',
                           'depth_upper', 'depth_lower'] + self.NUTRIENTS
            self.feature_names = [c for c in df.columns if c not in exclude_cols 
                                and df[c].dtype in ['float64', 'int64', 'float32', 'int32']]
        
        return df
    
    def _get_model(self):
        """
        Initialize model based on type selection
        
        Returns:
            sklearn-compatible model
        """
        p = self.params
        
        if self.model_type == 'xgboost':
            return xgb.XGBRegressor(
                objective='reg:squarederror',
                subsample=0.8,
                colsample_bytree=0.8,
                **p
            )
        elif self.model_type == 'lightgbm':
            return lgb.LGBMRegressor(
                objective='regression',
                subsample=0.8,
                colsample_bytree=0.8,
                **p
            )
        else:  # random_forest
            return RandomForestRegressor(
                max_depth=p['max_depth'],
                **p
            )
    
    def train(self, target_nutrients=None):
        """
        Train models for each nutrient
        
        Args:
            target_nutrients: List of nutrients to train (default: all)
        
        Returns:
            dict: Training results with CV scores
        """
        if target_nutrients is None:
            target_nutrients = self.NUTRIENTS
        
        # Engineer features
        X_train = self._engineer_features(self.train, is_train=True)
        X = X_train[self.feature_names].fillna(0)
        
        results = {}
        
        for nutrient in target_nutrients:
            if nutrient not in self.train.columns:
                continue
            
            # Prepare target
            y = self.train[nutrient].fillna(self.train[nutrient].median())
            
            # Scale features
            scaler = StandardScaler()
            X_scaled = scaler.fit_transform(X)
            self.scalers[nutrient] = scaler
            
            # Initialize model
            model = self._get_model()
            
            # Cross-validation
            cv = KFold(n_splits=self.cv_folds, shuffle=True, random_state=self.random_state)
            cv_scores = -cross_val_score(
                model, X_scaled, y, cv=cv,
                scoring='neg_root_mean_squared_error'
            )
            
            # Fit final model on full data
            model.fit(X_scaled, y)
            self.models[nutrient] = model
            
            # Calculate training RMSE
            train_pred = model.predict(X_scaled)
            train_rmse = np.sqrt(mean_squared_error(y, train_pred))
            
            results[nutrient] = {
                'cv_rmse': float(cv_scores.mean()),
                'cv_std': float(cv_scores.std()),
                'train_rmse': float(train_rmse)
            }
        
        self.training_results = results
        return results
    
    def predict(self, apply_zero_mask=True):
        """
        Generate predictions for test set
        
        Args:
            apply_zero_mask: Whether to apply TargetPred_To_Keep constraints
        
        Returns:
            DataFrame: Submission-ready predictions
        """
        # Engineer features for test set
        X_test = self._engineer_features(self.test, is_train=False)
        X_test = X_test[self.feature_names].fillna(0)
        
        predictions = {}
        
        for nutrient, model in self.models.items():
            if nutrient not in self.scalers:
                continue
            
            # Scale and predict
            X_scaled = self.scalers[nutrient].transform(X_test)
            preds = model.predict(X_scaled)
            
            # Apply zero-mask constraint
            if apply_zero_mask and self.zero_mask_dict:
                preds = np.array([
                    0.0 if (row['ID'], nutrient) in self.zero_mask_dict else pred
                    for pred, row in zip(preds, self.test.itertuples(index=False))
                ])
            
            # Ensure non-negative
            predictions[f'Target_{nutrient}'] = np.clip(preds, 0, None)
        
        # Build submission dataframe
        submission = self.test[['ID']].copy()
        for nutrient in self.NUTRIENTS:
            col = f'Target_{nutrient}'
            submission[col] = predictions.get(col, np.zeros(len(self.test)))
        
        return submission
    
    def save(self, path='models/'):
        """
        Save trained models and metadata
        
        Args:
            path: Directory to save models
        """
        Path(path).mkdir(parents=True, exist_ok=True)
        
        # Save individual models
        for nutrient, model in self.models.items():
            joblib.dump(model, f"{path}/{nutrient}_model.pkl")
        
        # Save scalers
        for nutrient, scaler in self.scalers.items():
            joblib.dump(scaler, f"{path}/{nutrient}_scaler.pkl")
        
        # Save metadata
        metadata = {
            'feature_names': self.feature_names,
            'zero_mask_dict': self.zero_mask_dict,
            'training_results': self.training_results,
            'config': {
                'model_type': self.model_type,
                'cv_folds': self.cv_folds,
                'params': self.params
            }
        }
        save_metadata(metadata, f"{path}/metadata.json")
    
    @classmethod
    def load(cls, path='models/'):
        """
        Load trained models from disk
        
        Args:
            path: Directory containing saved models
        
        Returns:
            SoilNutrientPredictor instance
        """
        import json
        
        # Load metadata
        with open(f"{path}/metadata.json", 'r') as f:
            metadata = json.load(f)
        
        # Create instance
        predictor = cls(
            model_type=metadata['config']['model_type'],
            cv_folds=metadata['config']['cv_folds'],
            **metadata['config']['params']
        )
        
        predictor.feature_names = metadata['feature_names']
        predictor.zero_mask_dict = metadata['zero_mask_dict']
        predictor.training_results = metadata['training_results']
        
        # Load models and scalers
        for nutrient in cls.NUTRIENTS:
            model_path = f"{path}/{nutrient}_model.pkl"
            scaler_path = f"{path}/{nutrient}_scaler.pkl"
            
            if Path(model_path).exists():
                predictor.models[nutrient] = joblib.load(model_path)
            if Path(scaler_path).exists():
                predictor.scalers[nutrient] = joblib.load(scaler_path)
        
        return predictor


def main():
    """Example usage - can be run as script"""
    import sys
    
    if len(sys.argv) < 3:
        print("Usage: python main.py <train.csv> <test.csv> [mask.csv]")
        return
    
    train_path = sys.argv[1]
    test_path = sys.argv[2]
    mask_path = sys.argv[3] if len(sys.argv) > 3 else 'TargetPred_To_Keep.csv'
    
    # Load data
    print("Loading data...")
    train_df = pd.read_csv(train_path)
    test_df = pd.read_csv(test_path)
    mask_df = pd.read_csv(mask_path) if Path(mask_path).exists() else None
    
    # Initialize and train
    print("Training models...")
    predictor = SoilNutrientPredictor(model_type='xgboost', cv_folds=5)
    predictor.load_data(train_df, test_df, mask_df)
    results = predictor.train()
    
    # Display results
    print("\nTraining Results:")
    for nutrient, metrics in results.items():
        print(f"  {nutrient}: CV RMSE = {metrics['cv_rmse']:.3f} Â± {metrics['cv_std']:.3f}")
    
    # Generate submission
    print("\nGenerating predictions...")
    submission = predictor.predict()
    submission.to_csv('submission.csv', index=False)
    print("âœ… Submission saved: submission.csv")
    
    # Save models
    predictor.save()
    print("ðŸ’¾ Models saved to models/")


if __name__ == "__main__":
    main()